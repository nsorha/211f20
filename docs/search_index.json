[["index.html", "Tools for Working with Data Chapter 1 About this book", " Tools for Working with Data Nicole Sorhagen, Ph.D. 2021-02-08 Chapter 1 About this book This book describes how to use R as a tool to work with data. You’ve mastered research methods, you’ve studied a psychological phenomenon, you’ve identified a need for a study, you’ve planned the study, you’ve developed and preregistered a hypothesis, you’ve collected the data – now what? This guide explains how to use RStudio for data management, data visualization, and data analysis. RStudio is a free and flexible statistics program. While there are some things in RStudio that can be done with point and click methods, most things are done with commands, or code. The commands use a programming language called R (which is based on another programming language called S). This guide focuses on aspects of R commands that are practical and relevant for a psychology student. While learning a programming language can be intimidating, it is my hope that the small incremental steps that I use in this guide will make the process of learning how to use RStudio similar to learning a statistics program with more point and click options (like JASP and SPSS). In addition to being useful for coursework and research, having a basic understanding of how to use R commands could help you become comfortable learning the basics of other programming languages for things like editing a website. The flexibility that RStudio provides often results in several ways to complete the same task. Because different options can be useful in different ways, I will often provide more than one way to run an analysis in this guide. This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License. "],["set-up-project-on-rstudio-cloud.html", "Chapter 2 Set up project on Rstudio Cloud", " Chapter 2 Set up project on Rstudio Cloud We will use Rstudio cloud on this website: https://rstudio.cloud. You must first make an Rstudio account by clicking the sign up button in the top right corner. (this is free) Then join our shared RStudio cloud workspace with the link that I sent you in the email titled ‘Rstudio cloud shared workspace’. You MUST join our shared workspace. I will be checking your work through this shared RStudio cloud workspace. Within this shared workspace, I will be able to see everyone’s project, but you will only be able to see your project and my project. Once you are in your Rstudio Cloud account… Expand the R studio cloud options by clicking on the 3 lines in the top left corner. Then select our course (which will be titled the name of course). If you cannot see this option – then you have not been added to our shared workspace. Once you are in the Class’s shared workspace, open a new project. The new project button is on the top right. Call this project your last name by clicking on the box that says ‘Untitled Project’ and typing your last name. Congrats! You have successfully set up your project on our class RStudio Cloud workplace! You will use this project for all of your R work for the rest of the semester. Next let’s learn a bit about the software environment and the R programming language. "],["introduction.html", "Chapter 3 Introduction 3.1 Make a project in RStudio cloud 3.2 WINDOWS IN RSTUDIO 3.3 NAVIGATING AND COMMANDS 3.4 DATA THINGS", " Chapter 3 Introduction This chapter introduces the RStudio cloud environment and describes how to import data into the RStudio cloud. R cannot handle typos and is case sensitive (‘Gender’ is not the same as ‘gender’). If your code will not run check for typos and caps. Related to this point, do not be afraid to copy and paste with using R. I often copy and paste code and replace variable or dataset names as needed. (This is one of the few times in education where copy and paste is OK!) 3.1 Make a project in RStudio cloud When using RStudio in the cloud you should first open a new project by clicking on the new project button in the top right hand side of the RStudio cloud website. Make sure you are in the shared class workspace when you do this - it should say PSYC 211 in the top right. If it does not - click on the hamburger button and then select the PSYC 211 shared workspace. 3.2 WINDOWS IN RSTUDIO RStudio has four panes or windows. 3.2.1 The Source Pane The source pane has several functions. The most important is that it is where you will write and run R commands. In this book we will use a R script files to do this, which are similar to text files. Another popular file type to write and run R commands in is a R markdown file, which can automatically create reports and manuscripts (see the appendix for more information about R markdown files). The source pane can also display datasets in a spreadsheet like format. The variable names will be listed at the top of each column, followed by the raw data. Multiple files can be opened in the source pane at the same time – there will be a tab for each opened file at the top of the pane. 3.2.2 The Console Pane The console or terminal pane shows a record of the tasks or analyses you have asked RStudio to complete. After you run a command from a R script in the source pane, the execution and output will appear in the console pane. The &gt; symbol in the last line of the console means that the console is ready for a command. If the &gt; symbol is ever missing, click in the last line of the console and hit the escape button on your keyboard. A &gt; symbol will then appear. While it is possible to type and run R commands in the console, typing commands in scripts in the source pane is better because the information in the console is not saved when RStudio is closed. When you reopen a project, the console will be reset. So, everything you want to save should be in the script. 3.2.3 The Environment And History Pane The environment pane shows what datasets and objects are currently defined in RStudio’s memory. The history tab contains a history of the R commands you’ve executed. This should be fairly redundant with your R scripts. 3.2.4 The Files/Plots/Packages/Help Pane This pane has several purposes. The file tab offers a way to open files in RStudio. The plots tab is where any plots you create will appear. The packages tab is where you can see what packages are currently loaded. You can also install and load packages from here. (Packages will be discussed in more detail in the next section.) Finally, the help tab displays documents to help you use R functions and packages. 3.3 NAVIGATING AND COMMANDS Use R scripts to type and run commands The first step of working in RStudio will always be to open a script. You can open an existing script by selecting it from the file tab in the files/plots/packages/help pane. R script files end in .R (the intro.R file, descripstats.R, and datavis.R files in the picture below are R script files. They contain the commands for chapters 1 to 3 of this guide. Or you could open an existing script by selecting FILE -&gt; OPEN FILE in the top bar menu and then navigate to the file location. Another option is to open a new script. Do this by selecting from the top bar menu: FILE -&gt; NEW FILE -&gt; R SCRIPT Another way to open a new script is by clicking on the green circle with a cross in the middle and then selecting R Script. To save a script, select from the top bar menu: FILE -&gt; SAVE AS. R is like a fancy calculator In its most basic form, you can think of R as a fancy calculator. For example, type the following equation into the script: 2+2 Then, while the cursor is still in the same line as the equation, click the run button to have R calculate the equation (or run this command). The command (2+2) and results (4) will appear in the console. You also could highlight the command you want to execute and then click on the run button. Another way to run a command in a script is by using the command (⌘) and enter keys on a mac (or control + enter on a PC). Functions are what makes the calculator fancy. In this book we will rarely calculate statistics by entering equations. Instead, we will use functions to tell R to use equations for us. For example, to find the mean of a set a scores (such as 2, 2, and 3) you could use the mean equation: M = (∑X)/N = = (2+2+3)/3 This equation would be entered into R as (2+2+3)/3. After you run this command, the answer (2.333333) would appear in the console. Alternatively, you could use the mean function to tell R that you would like it to calculate the mean of the set of scores. (i.e., mean(scores)). Functions act on arguments Functions always have parentheses at the end of their names, and inside the parentheses is where you tell R more about what you want it to do. The information inside the parentheses are called arguments. In the same way that verbs act on nouns, functions act on arguments. Information is stored in objects An object in R stores information. Functions are a kind of object that stores equations or actions. Data are stored in objects called vectors and datasets. The results of an analysis can also be saved as an object, as needed. Information is assigned to objects with assignment operators. This guide will use the arrow assignment operator (&lt;-). The equal sign can also be used as an assignment operator. Whatever is on the right of the assignment operator is assigned to whatever is on the left of the operator. For example, the command match &lt;- read_csv(\"matchdata.csv\") assigns the data in the file matchdata.csv to an object called match. All objects are listed in the environment. The environment can be cleared by clicking on the broom at the top of the environment tab. Or you could use the remove function. For example, rm(match) the object named match will no longer be listed in the environment. Use the $ symbol to access variables within a dataset object (an object with multiple variables in it). For example, match$age accesses the variable called age in the dataset object called match. Another way to tell R what data object a variable is in is with a pipe, or %&gt;%. Pipes are a way to write strings or series of functions more easily. You can think of it as saying “then”. For example match %&gt;% pull(age) is telling R to first go to the dataset in the match object then pull the variable called age. In order to use pipes (%&gt;%) a package called Tidyverse must be loaded – we will talk more about packages in the next section. R quirks Before moving on, we need to talk about a few of R’s quirks. First, R is case sensitive. R will treat mean differently than Mean. Because of this some experts suggest always using lowercase letters when naming things. This is something to keep in mind when you are choosing names for the objects that store your data. Second, R cannot handle typos. If you accidentally typed ‘men’ instead of ‘mean’, R will not know that you really meant mean. The predictive text feature of RStudio can help with this. This means that RStudio will suggest functions and objects as you type. Finally, R is ambivalent about spaces. R ignores redundant spaces in equations. For example, if you typed 2 +2 or 2+2 you would get the same answer as when you typed 2 + 2. But there cannot be spaces in object names (this is another thing to keep in mind when naming objects), and there cannot be spaces between a function and its parentheses. When you get an error message after you run a command, check for capitalization errors, typos, and spacing. Trial and error In the last sentence I said when you get an error, not if you get an error. This is because errors and having a command not run is inevitable. Errors are just a part of coding. It is even people’s jobs to find the errors in professional coder’s code. I often have to play around with a command a bit because it will not run. I like to copy and paste a command and then try something different, so I can see what I already have tried. So, don’t sweat the errors! It happens to everyone! Scripts should be a record of your analysis While trial and error is expected, it should be clear in a script which commands were eventually used and which were not. This is part of making your scripts useable by others and your future self. Another way to make scripts shareable is to include notes using the hashtag (#). R will not treat anything in the same line as a hashtag as a R command. In the current script we are working on, we could add organization and explanations to our current script by adding # R is like a calculator above the equations we have run so far and # Packages to indicate the next section. Note that the comments appear green in the script. At the very least, you should include enough comments so that someone else (or even yourself in a few months or so) could read your script and get the basic idea of what you did. Beyond that, the content and degree to which comments are included in scripts is a personal choice. For example, sometimes people note the results of an analyses after its command and others do not. Remember that the information in the console is not saved after RStudio is closed. So, including the results in the script can be useful when you reopen a project. 3.3.1 Packages Base R refers to the functions that automatically come with R. But many people build on top of Base R to make R better. The way they do this is through packages, which contain new R functions. There are thousands of packages that could be added on to base R. Packages are kind of like apps on your smartphone or computer in several ways. First, there are often several packages that can equally do the same thing. Just like there are several apps that would tell you the weather, there are several packages that would compute descriptive statistics. Moreover, some packages are more well-known and popular than others, just as some apps are really popular and others are not. Like the more popular apps, the well-known packages tend to be well supported, meaning that they are updated, maintained, etc. In this guide I try to focus on the more popular packages. Installing packages Another way that packages are like smartphone apps is that they both have to be installed first before they can be used. In RStudio, packages can be installed through point and click or with a command. Let’s first use the point and click method to install a package called tidyverse. Tidyverse was created by Hadley Wickham and his team with the aim of making various aspects of data analysis in R easier. It is actually a collection of packages, and they include a lot of functions that many people think of as essential for data analysis. To install a package using the point and click method, select from the top bar menu: TOOLS -&gt; INSTALL PACKAGES In the install packages window, type the name of the package you would like to install. For example, type “tidyverse” in the packages box. Then click INSTALL. Your screen should look like this when the installation is complete: Do not proceed until the console says the package has been installed. Another package we will be using a lot is the psych package. The psych package is a package for personality, psychometric, and psychological research. It has been developed at Northwestern University (maintained by William Revelle) to include useful functions for personality and psychological research. Let’s install this package with the install packages function. The basic setup of this function is: install.packages(“PackageName”) Replace PackageName with the name of the package that you want to install. Remember the quotation marks here! For example, install the psych package with the following command: install.packages(\"psych\") After you type (or copy and paste) this command into a script. Highlight it (or click in the same line as in the command) and then click on the run button (or use the run hot keys). Again, do not proceed until the console says the package has been installed. Loading packages In order to use a package, it must be loaded – just like in order to use an app on your smartphone, it must be opened. Another analogy is moving information from long term memory to working memory. You will need to load the packages you want to use each time you open RStudio. In RStudio, packages can be loaded with point and click or with a command. Let’s load tidyverse with the point and click method. To do this, first select the packages tab in the files/plots/packages/help pane. Then type “tidyverse” in the search bar (or scroll through the list). Then check the box next to tidyverse. The loading process will appear in the console immediately. The output in the console lists the 8 packages that make up Tidyverse (ggplot2, tibble, tidyr, readr, purr, dplyr, stringr, and forcats). Next let’s load the psych package with the library command. The basic set-up of this function is: library(PackageName) Replace PackageName with the name of the package. For example, load the psych package with this command: library(psych) After you run this code in the script, the loading process will appear in the console. Note that if you had already loaded Tidyverse, there will be a warning message telling you that the alpha function is masked from the ggplot2 package. This means that the psych package and the ggplot2 package both have a function called alpha. This is not a big deal. There is a way to tell R which alpha function you want to use. In summary, the first time you use a package, you need to install it. Once a package is installed, you will need to tell R that you want to use it by loading it. You only have to install a package in a project once. You will have to load a package every time you open RStudio and want to use it. 3.4 DATA THINGS Now that you have a sense of how to navigate and use commands in the RStudio program, let’s talk about data – how to enter it, how to open it, how to set it up correctly, and how to transform it. 3.4.1 Manual Entry Data can be added directly into RStudio. While it is more common to collect data in a spreadsheet or another programs and then open it in RStudio for data analysis, it is still important to review how to enter data directly into RStudio. This section will review how to go about doing this with an example about class tardiness. Let’s pretend that someone counted the number of times a week that five students were late for a class that meets on Mondays, Wednesdays, and Fridays. Here is their data: Let’s use the concatenate, or combine, function to assign the data to an object. The basic setup of this function looks like this: VariableName &lt;- c(X1, X2, X3, etc.) The VariableName &lt;- part of this command saves the data to an object. Replace VariableName with the name of the variable. Variable names should not have spaces or special characters in them. Naming variables is another part of making analyses shareable with others. Variable names should be informative but short (under 15 characters). The c is the combine (or concatenate) function In the parenthesis, list the data in a comma separated list. For example, here is the command to create the Student ID column: id &lt;- c(1, 2, 3, 4, 5) This command is telling R to save an object called studentid that is made up of this list of numbers (1, 2, 3, 4, 5) After you run this command, an object called ‘id’ will be listed in the environment pane. Next let’s assign the data for the number of times the students were late in the first week. Let’s call this variable latew1 (for late week 1, or the number of times students were late the first week of the study). Here is the command: latew1 &lt;- c(2, 0, 1, 2, 0) This command is telling R to save an object called latew1 that is made up of this list of numbers (2, 0, 1, 2, 0) After you run this command, an object called latew1 will be listed in the environment pane. Then enter the week 2 and 3 data with the following commands: latew2 &lt;- c(3, 0, 1, 2, 0) This command is telling R to save an object called latew2 that is made up of this list of numbers (3, 0, 1, 2, 0). latew3 &lt;- c(2, 1, 0, 3, 0) This command is telling R to save an object called latew3 that is made up of this list of numbers (2, 1, 0, 3, 0). These variables can be combined into one dataset object with the data.frame function. The basic setup of this function is: DatasetName &lt;- data.frame(Variable1, Variable2, etc) Replace DatasetName with what you want to call you data object. Like the variable names, dataset names should be informative but short (under 15 characters). In the current example the command would look like this: tardy &lt;- data.frame(id, latew1, latew2, latew3) This command is saying to assign the data in the id, latew1, latew2, latew3 objects to a dataset object named tardy After you run this command, there will be a new object in the environment called tardy. Note that the tardy object is listed under data and the individual variables are listed under values. 3.4.2 Upload data into RStudio Cloud When using RStudio in the cloud, you will first need to upload the data into your project on the website. To do this select the UPLOAD button in the files window. In the Upload Files window, click the CHOOSE FILE button and then navigate to the file on your computer. Then click the OK button. The data file should now be listed in the files section of RStudio. 3.4.3 Importing Data From Other Sources Often data is collected in a spreadsheet (like excel or google sheets) or through a survey manager (like Qualtrics or SurveyMonkey) and then is opened into RStudio. Before you can open a data file in RStudio, you need to know the format of the data file. See Table 1.1 for some common file extensions for data. Let’s open the data in the match.csv file (this is the data we will use in the data transformation section of this chapter, as well as in chapters 2 and 3). This data file contains data on 688 people currently on Match.com who were asked to complete a survey about themselves and their relationships. This survey included questions about whether or not they judge other people based on things like their appearance, possessions, etc., as well as demographic information such as highest level of education and age. (We will use this file to explore how judgmental people are on dating sites.) The first column contains arbitrary ID numbers to identify the participants The next column contains the participants’ ages The third column contains the participants’ highest level of education The following 12 columns contain the data from the judgement questions (0 = no; 1 = yes) The remaining variables are not relevant to the current chapter, so they are not defined here. note that this data is a subsample of a real data collected through the Kinsey Institute and match.com You can open data in RStudio using a point and click method or using a command. The point and click method of importing data in RStudio is simple and convenient. This method is accessed by clicking on the import dataset button in the environment and history pane. Select one of the “from text” options if the data is formatted in a .csv or .txt file. (I tend to use the readr option here – but either works fine). Select the “from excel” option if the data is in a .xls or .xlsx format. Select the “from SPSS” option if the data is in a .sav format. Select the “from SAS” option if the data is in a .sasb7dat format. Select the “from Stata” option if the data is in a .dta format. Since the data is in a .csv file, let’s select the readr option. The following window will open: Click on the BROWSE button. Then navigate to your file. Inspect the data preview. You should not need to change anything from the default import options. Before you click import, I suggest you copy the text in the code preview box. After you click import, paste it into your current script. This way you can easily reassign the datafile to an object if needed by running that command. After you click import, your screen will look like this: The spreadsheet view of the match dataset will automatically open in the source pane. The variable names are listed at the top of each column, followed by the raw data. Scroll to the right to see all of the data. The object containing the match data will be listed in the environment. We can see that this dataset has 688 observations and 37 variables. You can close the spreadsheet of the match dataset in the source pane by clicking on the x in the tab. If you would like to see the data again, click on the dataset’s object name (in this example match) in the environment tab and the spreadsheet will reopen in the source pane. You can also open csv data with read_csv function. The read_csv function is part of the readr package, which is part of tidyverse. So if tidyverse is loaded, you do not need to load readr. But if it is not, you can load it with this: library(readr) The basic set up of the read_csv function is: DataObject &lt;- read_csv(\"FileName.csv\") This command is telling R to assign the data in a csv file named FileName to an object called DataObject. Replace DataObject with your preferred name for the object containing the data Replace FileName with the name of the file your data is in For example, to assign the match data to an object called match, the command would look like this: match &lt;- read_csv(\"matchdata.csv\") This command is telling R to assign the data in the matchdata.csv file to an object called match. Note that this is the same thing as what you copied and pasted during the point and click method. 3.4.4 Data Set Up There are several types of data, or scales of measurement, for variables in RStudio. The big ones for us will be: double, which are numbers with decimals, positive/negative, fractions; integer, which are positive whole numbers; character, which are numbers and words, but as text; and factor which are categorical variables, with specific labels. The glimpse function, which is part of the tidyverse package, gives a nice summary of a dataset including the variable types for all the variables. In order to use this function, the tidyverse package should be loaded (see the loading packages section above). The basic layout of the glimpse function is: glimpse(DataObject) Replace DataObject with the name of the data object For example, to look at the data types of the variables in the match dataset use this command: glimpse(match) After this command is run, the output in the console looks like this: The first column lists the names of all of the variables in the dataset. Next, in gray, are the variable types. The dbl stands for double, which is numbers with decimals, positive/negative, fractions. The chr stands for character, or text, data. After the data type, the remainder of the row lists the raw data of the variable. The data type of a variable can be changed with the as.* function. The * is a placeholder for the different types of data. For example, as.factor(VariableName), as.character(VariableName), and as.integer(VariableName). 3.4.5 Data Transformation Often times there is a need to change data in some way, such as combining variables together to create a summary score. These changes to data are called data transformations. This section covers how to use a RStudio to preform two common types of data transformations: computing a new variable and recoding values. Let’s pretend you were interested in how judgmental people are on dating sites. We can use the match data from the Importing Data section to do this. Remember that the participants completed items about whether or not they judge other people based on certain things. Here are the judgment items: Let’s create a total judgment score by summing all of the separate judgment items. To do this we will use the mutate function. The mutate function is part of the tidyverse package, so you should load tidyverse if you have not done so already (see the loading packages section above). The basic setup of this function is: mutate(NewVariableName = New variable equation) Replace the NewVariableName with the variable name you want. There cannot be spaces or symbols in a variable name. And remember that R is case sensitive, so be mindful of your use of upper and lower case letters. Then list the equation after the equal sign. In addition, you need to tell R which data object to work with. Adding this information makes the command look like this: DataObject %&gt;% mutate(NewVariableName = New variable equation) The %&gt;% is called a pipe. Pipes are a way to write strings or series of functions more easily. You can think of it as saying “then”. In this example you can read the command as saying “use the data in the object called DataObject then mutate the variables in that dataset by creating a new variable called NewVariableName which is defined by this equation. Pipes are part of the tidyverse package. Finally, you need to tell R to save the new variable. Do this with the assignment operator (&lt;-). If you want the new variable to be added to a dataset in an existing object, then tell R to reassign the object as itself like this: DataObject &lt;- DataObject %&gt;% mutate(NewVariableName = New variable equation) Or you could tell R to save the variables in the existing dataset plus the new variable by assigning the work to a new object: NewDataObject &lt;- DataObject %&gt;% mutate(NewVariableName = New variable equation) This will save all of the variables in the object named DataObject, plus the variable named NewVariableName into a new object called NewDataObject. After this command is run, NewDataObject will be listed in the environment. For our current example, let’s add the total judgment score variable to the match dataset. Here is the command to do so: match &lt;- match %&gt;% mutate(judgmt = jdgoutfit + jdgjob + jdgborn + jdglive + jdgcar + jdguni + jdgdiet + jdgdrink + jdgteeth + jdgposts + jdggramr + jdgsparet) The match &lt;- part of the command tells R to resave the new variable in the object called match. The match %&gt;% mutate part tells R to use the data in the object called match then mutate… Inside the parenthesis is the new variable, which I called judgmt and its equation, which is the sum of the 12 judgment items. After this command is run, there will be 38 variables in the dataset object called match (remember that originally there were 37). If you want, you can see the new variable by opening the dataset (click on the object listed in the environment) and then scroll to the right. The new variable will be in the last column. Next let’s review how to recode a variable. Still using the match data, let’s pretend you wanted to recode the number of second dates that participants reported having in the last year (This variable is called scnddates). We can see in the codebook that the response options range from 0 to “over 20”. Let’s recode this variable into categories. Let’s say that we want to know the number of people that had 0 second dates, and then 1 or 2 second dates, 3 or 4 second dates, and then 5 to 9, 10 to 14, 15 to 19, and 20+. We can do this with the cut function, which is a pretty cool function of the tidyverse package. Here is the basic command: DataObject &lt;- DataObject %&gt;% mutate(VariableCat = cut(x = Variable, breaks = c(-Inf, N, Inf), labels = c(\"-Infinity\", \"N” \"Infinity\"))) Replace DataObject with the name of the object containing the data that you are working with. Replace VariableCat with whatever you want the new variable to be called. I like to use the original variable name followed by “Cat” (for categorical). Replace Variable with the name of the variable you are recoding. Replace the entries in the breaks parenthesis with where you would like to cut the original variable. First list the lowest value of the first category – I like to use -Inf (where Inf stands for infinity). Then list the highest value of each category separated by commas (i.e., the second number listed is the highest value of the first category, the third number is the highest value of the second category, and so on). I like to end with Inf, you could also end with the highest value of the last category. Replace entries in the in labels parenthesis with the corresponding labels that you listed in the breaks. In the match example this command would look like: match &lt;- match %&gt;% mutate(scnddatesCat = cut(x = scnddates, breaks = c(-Inf, 0, 2, 4, 10, 15, Inf), labels = c(\"0\", \"1 or 2\", \"3 or 4\", \"5 to 9\", \"10 to 15\", \"over 15\"))) After this command is run from the script, your screen will look like this: There are now 39 variables in the dataset object called match. Finally, a common type of data transformation in psychology is recoding responses made on a Likert scale. It is common in psychology to have some items of a survey negatively worded and others positively worded. When this happens, we need all of the items to go in the same direction in order to analyze the data. We do this by reverse coding the items going in one direction – so that a high score on all of the items reflect the same thing. To reverse code in RStudio, we will use the mutate and recode functions of tidyverse. The basic form of this command is: DataObject &lt;- DataObject %&gt;% mutate(ItemR =recode(Item,OldValue1= NewValue1,OldValue2= NewValue2, Etc.) Replace DataObject with the name of the data object Replace ItemR with whatever you want the new variable to be called. I like to use the original variable name followed by “R” (for reverse). Replace Item with the name of the item that needs to be reverse coded. Next is a list of the old and new variables. On the left is the old variable and it must be in back ticks (`) when it is a number. String (AKA text) variables should be in quotes (\"). On the right is the new value. For example, if responses to the items of a measure were on a 5-point Likert scale and item 2 needed to be reverse coded, the command would look like this: data &lt;- data %&gt;% mutate(item2r =recode(item2,1= 5,2= 4,3= 3,4= 2,5= 1)) The data object is called data item2r will be the name of the new variable. item2 is the item that is being recoded. All of the 1’s in the data will be recoded as 5’s All of the 2’s in the data will be recoded as 4’s All of the 3’s will remain 3’s All of the 4’s in the data will be recoded as 2’s All of the 5’s in the data will be recoded as 1’s "],["picturing-data.html", "Chapter 4 Picturing Data 4.1 RESEARCH QUESTION 4.2 METHOD 4.3 DATA ANALYSIS OVERVIEW 4.4 OPEN DATA 4.5 FREQUENCY DISTRIBUTION 4.6 HISTOGRAMS 4.7 BOXPLOTS 4.8 SCATTERPLOTS 4.9 BAR GRAPH", " Chapter 4 Picturing Data The first step of data analysis is to look at your data with graphs. Graphs of data are also used to convey the results of a study. In this chapter you will learn how to use RStudio to create frequency distributions, histograms, boxplots, scatterplots, and bar graphs. 4.1 RESEARCH QUESTION How often do people judge others? Let’s say that we are interested in how judgmental people are on dating sites. Moreover, let’s pretend to be interested in whether people’s tendencies to judge are related to their age and education. 4.2 METHOD Pretend that you worked with Dr. Amanda Gesselman and her colleagues at the Kinsey Institute to create self-report survey items about how often participants judge others. These items were added to a longer semi-proprietary survey about the attitudes and behaviors of single people in the US. This survey was sent to certain people with accounts on match.com. The people were selected so that the demographics match the most recent US census, with oversampling for gay, lesbian, and bisexual participants. Thousands of people completed the survey. We will use a random sample of 688 people from the total sample here. 4.3 DATA ANALYSIS OVERVIEW The data is in file matadatacmplt.csv. This is the same dataset that we used in chapter 1 - except that the total judgement variable is included matadatacmplt and not in the matchdata.csv file. (note that this data is a subsample of real data collected through the Kinsey Institute and Match.com) • The first column contains arbitrary ID numbers to identify the participants • The next column contains the participants’ ages • The third column contains the participants’ highest level of education • The following 12 columns contain the data from the judgement questions (0 = no; 1 = yes) • The judgmt column contains the total number of times the participants reported judging others on the 12 items. (this is the variable you made in the data transformation section of chapter 1) • The remaining variables are not relevant to the current chapter, so they are not defined here. See the codebook (matchcodebook.xlsx) for more information. After we open the data in RStudio, we will look at it with tables and graphs. Frequency distributions, histograms and boxplots help you get to know your data and check for errors. Frequency distributions are tables that list all possible values or categories of a variable and their frequency within a study or data set. Histograms are a graph of a frequency table, where all possible values of a variable are on the x-axis and the frequency of each value is on the y-axis. Boxplots (AKA box and whisker plots) are another way to graphically represent data. The y-axis of a boxplot lists the possible values of a variable. The box represents the Interquartile range (IQR), which is the middle 50% of scores. The line withing the box shows the median. The highest and lowest scores are represented as whiskers if they are within a certain range (1.5 * IQR). If they are outside of this range, they are represented with a dot because they are considered an outlier. A traditional boxplot can be modified to include dots that represent individual data points (this is becoming increasingly popular because it encourages research transparency). Another popular modification is a violin element, which shows the distribution of the individual data points. Scatterplots and bar graphs help show your results to others. Scatterplots show the association between two variables. One variable is plotted on the y-axis and the other is plotted on the x-axis. Each dot represents one participant, measured on the two variables. Bar graphs show the association between continuous and categorical variables. They present the means of a continuous variable separately for each level of a categorical variable. Error bars representing the 95% confidence interval (CI) for each mean should be included in order to represent the variability in the data. 4.4 OPEN DATA First assign the dataset to an object called match (or whatever you prefer). You can do this with the point and click method by selecting from the environment menu: IMPORT DATASET -&gt; FROM TEXT (READR) Click the BROWSE button and then navigate to your file’s location on your computer. Then click OPEN and then IMPORT. Or you could use the following code: match &lt;- read_csv(\"matchdatacmplt.csv\") This command is telling R to assign the data in the matchdatacomplt.csv file to an object called match. Your screen should look like this after the data is imported: 4.5 FREQUENCY DISTRIBUTION Let’s first create a frequency distribution of the education variable. One way to do this is with the count function, which is part of the tidyverse package. This function counts the unique values of a variable. In order to use the count function the tidyverse package must be loaded. If it is not already loaded, load it with this command: library(tidyverse) (or you could use the point and click method described in chapter 1) Here is the command to use the count function: DataObject %&gt;% count(VariableName) This command is saying to first go to the dataset named DataObject and then count all of the unique values of the variable VariableName. Replace DataObject with the name of the object that is storing the data. Replace VariableName with the name of the variable you would like to count. In this example, use the following command to create a frequency table of the education variable: match %&gt;% count(edu) This command is telling R to use the data is the match object and count the variable called edu. The frequency table will appear in the console. Your screen should look like this: The first column of the frequency table lists the levels of the education variable. The NA means not available. This means that this data is missing. The categories in the frequency table are listed in alphabetical order by default. It is possible to change the order in which RStudio lists the levels of a factor. In the current example it would be nice for the education categories to appear in progressive order. We can do this by using the reorder function, which is part of base R, so no packages need to be loaded in order to use it. Here is what the function looks like in general: DataObject$VariableName &lt;- factor(DataObject$VariableName,levels = c(\"Level1Name\", \"Level2Name\", \"Level3Name\", \"Level4Name\")) The DataObject$VariableName &lt;- saves the reordered levels of the variable called VariableName in the data object called DataObject. Without this the information will not be saved for future use. The factor(DataObject$VariableName,levels = c(\"Level1Name\", \"Level2Name\", \"Level3Name\", \"Level4Name\")) part is what reorders the levels of the factor. The levels should be listed in the new order. Replace each DataObject with the name of the object that is storing the data. Replace each VariableName with the name of the variable you would like to count. Replace the Level1Name with the name of the factor level that you would like to appear first, the Level2Name with the name of the factor level that you would like to appear second, etc. In the current example this looks like this: match$edu &lt;- factor(match$edu,levels = c(\"high school degree or less\", \"two year degree or some college\", \"bachelors degree\", \"graduate or professional degree\")) After you run the reorder command, rerun the count command. The order of the educational attainment categories in the table will now be displayed in sequential order. The frequency table show that there are 86 people with a high school degree or less; 244 people with an associate, vocational, technical degree or some college, 245 people with a bachelor’s degree, and 112 people with a graduate degree. There is missing data for one person. The total sample is 688. Often, we want to know the percentages of the total sample that the raw frequencies represent. While we could add code to the count function in order to get this information (see appendix for this code), an easier way to get this information is to use a the freq function of the descr package. In order to use this function the descr package must first be installed. Here is the command to install it: install.packages(\"descr\") (or you could use the point and click method described in chapter 1) Once the package is installed, load it with this command: library(descr) (or you could use the point and click method described in chapter 1) The freq command looks like this: freq(DataObject$VariableName) Replace DataObject with the name of the object that is storing the data. Replace VariableName with the name of the variable you would like to count. In the current example it would look like: freq(x = match$edu) Here is what your screen should look like: Note that the plot on the left-hand side is not very informative. You can suppress this by adding plot = FALSE in the parenthesis: freq(x = match$edu, plot = FALSE) The percent column converts the raw frequencies to percentages of the total sample including any missing data. The valid percent column converts the raw frequencies to percentages of the number of non-missing data points. Because there is one data point missing in this example, the percent and valid percent columns are slightly different. 4.6 HISTOGRAMS Next let’s create a histogram of the age variable. One way to do this is with the hist function, which is part of base R and looks like this: hist(DataObject$VariableName) Replace DataObject with the name of the object that is storing the data. Replace VariableName with the name of the variable you would like to plot. Because it is part of base R, no package needs to be loaded in order to use it. In this example the command to create a histogram of the age variable is: hist(match$age) The plot will appear in the bottom left pane. This histogram provides a more detailed picture of the data because each bar represents the frequency of two years, while the bin width of the histogram that was created with the hist function was 5. Ggplot is typically taught with the analogy of a globe that is built one layer at a time. It starts with a world of only ocean (no land). Then “layers” of land, colors, terrains, legends, etc. are progressively added with additional commands. Use a plus sign (+) to add the layers. Let’s break down the command we used to create the histogram of age to see the step-by-step, layer-by-layer progression of commands that create a ggplot. First the data set object and the ggplot function creates the blank globe with only ocean: The aesthetic mapping adds the variable data: Then the geom_histogram adds the graph: Throughout this book we will add on different types of graphs and details to ggplot graphs. 4.7 BOXPLOTS Let’s use a boxplot to look at the data of the judgment variable. This can be done through base R with the boxplot function: boxplot(DataObject$VariableName) Replace DataObject with the name of the object that is storing the data. Replace VariableName with the name of the variable you would like to plot. In this example it would look like: boxplot(match$judgmt) After this command is run, the graph will appear in the plots pane: The boxplot shows that most people in our sample are not very judgmental – 50% of the data is between 1 and 5 on a 12-point scale. However, the top whisker and outlier point suggest that there are few people in the sample who are very judgmental. As with the histogram, many people prefer to use ggplot to create a boxplot because it is customizable. The command to do this is: DataObject %&gt;% ggplot(aes(x = \"\", y = VariableName)) + geom_boxplot() Replace DataObject with the name of the object that is storing the data. Replace VariableName with the name of the variable you would like to plot. The \"\"is a placeholder because we want the frequency of Y (rather than adding a second variable). In the current example the command looks and resulting graph likes this: match %&gt;% ggplot(aes(x = &quot;&quot;, y = judgmt)) + geom_boxplot() This looks very similar to the base R boxplot. However, unlike in base r, additional elements and details can be added to the boxplot when using ggplot. For example, individual data points can be added to a boxplot with the geom_point option: DataObject %&gt;% ggplot(aes(x = \"\", y = VariableName)) + geom_boxplot() + geom_point() Replace DataObject with the name of the object that is storing the data. Replace VariableName with the name of the variable you would like to plot. The geom_point()tells R to add the raw data to the boxplot. Add position = \"jitter\", color = \"grey\" in the geom_point parentheses(i.e., geom_point(position = \"jitter\", color = \"grey\") if there are a lot of participants in the dataset because it helps with readability. It is telling R to jitter, or to add random noise to the data points. Without it the data points would overlap each other. The color adds color to an object. I picked grey here, but you can pick any color (yellow, blue, etc.). The point of the color is to make it easier to tell the difference between the data and the rest of the graph. Because the N for the match data set is on the larger side, we should add the jitter and color to the data points. Here is what the command and graph look like for the current example: match %&gt;% ggplot(aes(x = &quot;&quot;, y = judgmt)) + geom_boxplot() + geom_point(position = &quot;jitter&quot;, color = &quot;grey&quot;) This graph shows us that there are more data points at the bottom of the judgmental distribution (less judgmental) than the top. Instead of the individual data points, the distribution of data points can be added to a boxplot with the geom_violin option like this: DataObject %&gt;% ggplot(aes(x = \"\", y = VariableName)) + geom_boxplot() + geom_violin(alpha = .5) Replace DataObject with the name of the object that is storing the data. Replace VariableName with the name of the variable you would like to plot. The alpha = . 5 tells R to make the violin distribution transparent so that the boxplot is still visible. The alpha makes objects transparent. I pick .50, which is half transparent, .75 would be darker and .25 would be lighter. Which looks like this in the current example: match %&gt;% ggplot(aes(x = &quot;&quot;, y = judgmt)) + geom_boxplot() + geom_violin(alpha = .5) In this example, the violin plot does a better job at showing that there are more people at the lower end of the distribution of judgmental scores. 4.8 SCATTERPLOTS Next let’s create a scatterplot between the judgement and age variables. One way to do this is with the plot function of base R, which looks like this: plot(DataObject$Variable1Name, DataObject$Variable2Name) Replace DataObject with the name of the object that is storing the data. Replace Variable1Name with the name of one of the variables and Variable2Name with the other. In the current example, the plot function would look like this: plot(match$judgmt, match$age) The ggplot command for a scatterplot is: DataObject %&gt;% ggplot(aes(x = Variable1Name, y = Variable2Name)) + geom_point() Replace DataObject with the name of the object that is storing the data. Replace Variable1Name with the name of one of the variables and Variable2Name with the other. In the current example this would look like this: match %&gt;% ggplot(aes(x = age, y = judgmt)) + geom_point() Here you can see a slight negative trend, such that the younger people in the sample have slightly higher judgmental scores than the older people in the sample (the correlation coefficient (r) = -.15). 4.9 BAR GRAPH Finally, let’s make a bar graph of an association between a categorical and continuous variable. For this example, let’s look at the association between education and how many times participants reported judging others. To do this we will make a graph of the mean judgment score for each level of education. Since we are plotting a summary statistic (a mean) instead of raw data (as we did with the histogram and scatterplot), we have to set up our ggplot command a little differently. After specifying the data object and aesthetic, we will use the stat_summary function to tell R that we want to plot the means with a bar graph. Here is what this looks like: DataObject %&gt;% ggplot(aes(x = Variable1Name, y = Variable2Name)) + stat_summary(fun = mean, geom = \"bar\") Replace DataObject with the name of the object that is storing the data. Replace Variable1Name with the name of the categorical variable and Variable2Name with the name of the continuous variable. Here is the command and graph for the current example: match %&gt;% ggplot(aes(x = edu, y = judgmt)) + stat_summary(fun = mean, geom = &quot;bar&quot;) The graph includes the mean for the one person who did not complete the education question of the survey. The NA category can be dropped by adding the drop.na function to the ggplot plot by adding drop_na(Variable1Name) %&gt;%. In the current example this looks like: match %&gt;% drop_na(edu) %&gt;% ggplot(aes(x = edu, y = judgmt)) + stat_summary(fun = mean, geom = &quot;bar&quot;) Finally, 95% confidence intervals around each mean should be added. Use another stat_summary function to tell R that you would like to plot the standard of the mean with error bars by adding this line stat_summary(fun.data = mean_se, geom = \"errorbar\", width = .3). In the current example this looks like: match %&gt;% drop_na(edu) %&gt;% ggplot(aes(x = edu, y = judgmt)) + stat_summary(fun = mean, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = .3) Finally, if you would like to present this graph professionally, the additional options can be added to the ggplot command to adjust color and labels. The theme_classic function will change the background to white, remove gridlines and add x and y axis lines. The xlab and ylab function will add labs to the respective axis. match %&gt;% drop_na(edu) %&gt;% ggplot(aes(x = edu, y = judgmt)) + stat_summary(fun = mean, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = .3) + theme_classic() + xlab(&quot;Education&quot;) + ylab(&quot;Mean Judgement Score&quot;) This graph shows that the mean judgmental score increases with educational attainment. "],["descriptive-statistics.html", "Chapter 5 Descriptive Statistics 5.1 RESEARCH QUESTION 5.2 METHOD 5.3 DATA ANALYSIS OVERVIEW 5.4 OPEN DATA 5.5 DESCRIPTIVE STATISTICS 5.6 DESCRIPTIVE STATISTICS BY GROUP", " Chapter 5 Descriptive Statistics After you look at your data, the next step in the data analysis process is to compute descriptive statistics. In this chapter you will learn how to use RStudio to compute measures of central tendency and variability. 5.1 RESEARCH QUESTION Let’s continue with the match.com example from chapters 1 and 2. In this example we are pretending to be interested in the judgement of people on dating apps – its frequency and its relation to age and education. In the last chapter we looked at tables and graphs that told us a bit about the people in our sample in regard to their education, age, and how often they self-report judging others. In this chapter we will add numbers to this understanding with descriptive statistics. 5.2 METHOD Pretend that you worked with Dr. Amanda Gesselman and her colleagues at the Kinsey Institute to create self-report survey items about how often participants judge others. These items were added to a longer semi-proprietary survey about the attitudes and behaviors of single people in the US. This survey was sent to certain people with accounts on match.com. The people were selected so that the demographics match the most recent US census, with oversampling for gay, lesbian, and bisexual participants. Thousands of people completed the survey. We will use a random sample of 688 people from the total sample here. 5.3 DATA ANALYSIS OVERVIEW The data is in file matadatacmplt.csv. This is the same dataset that we used in the introduction chapter - except that the total judgment variable is included matadatacmplt and not in the matchdata.csv file. The first column contains arbitrary ID numbers to identify the participants The next column contains the participants’ ages The third column contains the participants’ highest level of education The following 12 columns contain the data from the judgement questions (0 = no; 1 = yes) The judgmt column contains the total number of times the participants reported judging others on the 12 items. (this is the variable you made in the data transformation section of the introduction chapter) The remaining variables are not relevant to the current chapter, so they are not defined here. After the data is opened and you have looked at the data with tables and graphs, the next step is to compute descriptive statistics. Descriptive statistics refer to measures of central tendency and variability. Measures of central tendency describe the middle of a distribution of scores. These statistics include the mean, median, and mode. Measures of variability describe the spread or dispersion of the distribution. The standard deviation, variance, and range are common measures of variability. Descriptive statistics are often calculated for the whole sample and also separately for different levels of a categorical variable. 5.4 OPEN DATA First assign the dataset to an object called match (or whatever you prefer). You can do this with the point and click method by selecting from the environment menu: IMPORT DATASET -&gt; FROM TEXT (READR) Click the BROWSE button and then navigate to your file’s location on your computer. Then click OPEN and then IMPORT. Or you could use the following code: library(readr) match &lt;- read_csv(\"matchdatacmplt.csv\") This command is telling R to assign the data in the matchdatacomplt.csv file to an object called match. Your screen should look like this after the data is imported: 5.5 DESCRIPTIVE STATISTICS This section reviews two ways to compute descriptive statistics in RStudio. First, we will compute descriptive statistics using the psych and tidyverse packages. Then we will review how to use base R to compute them. 5.5.1 Psych and Tidyverse Let’s first load tidyverse, so that we can use pipes (%&gt;%) in our commands: library(tidyverse) (or you could use the point and click method described in chapter 1) Then load the psych package: library(psych) (or you could use the point and click method described in chapter 1) Let’s compute descriptive statistics for the judgmental and age variables with the describe function of the psych package. This function was made to compute common descriptive statistics in psychology research. Here is the command: DataObject %&gt;% describe() Replace DataObject with the name of the object that is storing the data In the current example the command looks like this: match %&gt;% describe() This command is saying to first go to the dataset named match and then describe it After this command is run, your screen should look like this: The first column lists the variable names. The vars columns numbers the variables. The next column reports the number of valid data points for each variable. The next is the mean. The sd in the following column stands for standard deviation. After the standard deviation is the median. In the next few columns trim stands for the trimmed mean; mad stands for median absolute deviation; min stands for minimum; max stands for maximum. The se in the last column stands for standard error. You can see that every variable in the match data set is listed in the first column. However, the statistics for some of the variables are meaningless, like the statistics listed in the id row for example (the mean of an arbitrary ID number doesn’t mean anything). Also, descriptive statistics cannot be calculated for categorical variables (which can result in warnings). The select command can be added in order to calculate descriptive statistics for only certain variables (use the pull function to select one variable). For example, this is the command to select only the age and judgment variables: match %&gt;% select(age, judgmt) %&gt;% describe() This command is telling R to use the data in the match dataset then select the age and judgment variables then describe those variables. After you run that command, the follow will appear in the console: The mean age is 36.18 and the median is 33.5. The mean being greater than the median suggests a positive skew to the data because the older aged people in the tail of the distribution pull the mean age (36.18) to be greater than the 50% mark (33.5). The most frequent age is 24. The standard deviation is 12.45 years, which tells us most people are between 23 to 45 (36 +/- 12). The ages range from 18 to 60 years old. The mean judgmental score is 3.54, the median is 3, and the mode is 0 – suggesting that most people in the sample do not judge people very often (or they don’t admit to it at least). The standard deviation is 3.11, so most people in our sample said they judge other people based on between 0 to 6 things. The range was 0 to 12. 5.5.2 Base R You could also use base R to compute descriptive statistics. There are functions to compute individual statistics. For example, the mean function calculates the mean of a variable. It looks like this: mean(DataObject$VariableName) Replace DataObject with the name of the object that is storing the data. Replace VariableName with the name of the variable. In the current example, this command would calculate the mean age of everyone in the sample: mean(match$age) The following will appear in the console after that is run from your script: Table 3.1 lists R commands for other common statistics. Replace DataObject with the name of the object that is storing the data and replace VariableName with the name of the variable. Add na.rm = TRUE after in the parenthesis if there is missing data in the variable (i.e. mean(DataObject$VariableName, na.rm = TRUE)) The summary function (summary(DataObject$VariableName)) calculate minimum, 25% point, median, mean, 75% point, and maximum point at the same time. For example, use the following for code to use the summary function of the judgment variable: summary(match$judgmt) After this is run from the script, the following will appear in the console: 5.6 DESCRIPTIVE STATISTICS BY GROUP Next let’s compute the judgment descriptive statistics separately for each level of the education variable. To do this let’s use the describeBy function of the psych package, which reports basic summary statistics by a grouping variable. Here is the command: DataObject %&gt;% pull(Variable1Name) %&gt;% describeBy(DataObject$Variable2Name) Replace DataObject with the name of the object that is storing the data. Replace Variable1Name with the name of the variable that you want to describe. Use the select function instead of the pull function with more than one variable. Replace Variable2Name with the name of the grouping variable In the current example the command and results looks like: match %&gt;% pull(judgmt) %&gt;% describeBy(match$edu) ## ## Descriptive statistics by group ## group: bachelors degree ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 245 3.94 3.15 3 3.62 2.97 0 12 12 0.75 0.01 0.2 ## ------------------------------------------------------------ ## group: graduate or professional degree ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 112 4.29 3.29 4 3.94 2.97 0 12 12 0.79 -0.13 0.31 ## ------------------------------------------------------------ ## group: high school degree or less ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 86 2.27 2.55 2 1.86 2.97 0 12 12 1.37 1.85 0.28 ## ------------------------------------------------------------ ## group: two year degree or some college ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 244 3.25 3 3 2.83 2.97 0 12 12 1.08 0.7 0.19 In this table we can see that the mean and median judgmental scores increase with educational attainment (high school or less: mean = 2.27, median = 2; two-year degree or some college: mean = 3.25, median = 3; bachelor’s degree: mean = 3.94, median = 3; graduate or professional degree: mean = 4.29, median = 4). The most common judgmental score was zero for the participants with bachelor’s degrees or less. The mode of the participants with graduate or professional degree was 3. Based on the standard deviations, there were slightly more variability in the scores at the higher level of education (high school or less = 2.55; two-year degree or some college = 3; bachelor’s degree = 3.15; graduate or professional = 3.29). However, the range was the same for all levels of education. "],["measurement.html", "Chapter 6 Measurement 6.1 RESEARCH QUESTION 6.2 METHOD 6.3 DATA ANALYSIS OVERVIEW 6.4 OPEN DATA 6.5 INTERNAL RELIABILITY 6.6 CRITERION, CONVERGENT, AND DISCRIMINANT VALIDITY 6.7 APA-STYLE WRITE-UP", " Chapter 6 Measurement In this chapter we will learn how to use RStudio to assess the construct validity of a measure. We will use scatterplots and correlation coefficient, or r, to evaluate a measure’s reliability and validity. 6.1 RESEARCH QUESTION Spatial anxiety refers to anxiety about performing spatial tasks like navigation or mentally rotating objects (Lawton, 1994; Ramirez, Gunderson, Levine, &amp; Beilock, 2013). Jing Tian, Su Dam, and Liz Gunderson were interested in how spatial anxiety relates to gender differences in the ability to make number line estimations. They asked approximately 200 elementary school students to complete the Child Spatial Anxiety Questionnaire (CSAQ), which is a measure of spatial anxiety for children. The measure (which can be seen below) consists of 8 items that ask children how nervous they would feel during situations involving spatial tasks. Children responded using five smiley faces displaying emotions ranging from “not nervous at all” to “very, very nervous”. 6.2 METHOD Let’s pretend that you completed a study to test the construct validity of the CSAQ. That is, you wanted to know if the CSAQ is a good measure of children’s spatial anxiety. To do this you contacted the same 200 elementary school students one week after Jing Tian, Su Dam, and Liz Gunderson collected their data, and you collected additional data that will allow you to test whether the CSAQ is reliable (consistent) and valid (accurate). There are three types of reliability. No additional data collection is needed to test the internal reliability of the CSAQ, or the consistency of responses within the survey. Test-retest reliability refers to the consistency of a measure over time. In order to assess whether the CSAQ is consistently measuring children’s spatial anxiety over time, you gave the children the CSAQ again. Interrater reliability refers to the consistency of observations. As you are interested in establishing the construct validity of a self-report measure, this is not relevant here. There are three empirical ways to assess validity. Criterion validity refers to whether a measure is related to relevant behavioral outcomes. To test the criterion validity of the CSAQ you measured a behavioral indicator of spatial anxiety by recording the children’s heart rate during a 15 minute geography lesson. Convergent validity refers to whether a measure is related to similar measures. To test this, you gave the children a different measure of spatial anxiety. You adapted Lawton’s (1994) Spatial Anxiety Scale to be appropriate for children. Discriminant validity refers to whether a measure is not related to dissimilar measures. You gave the children a measure of general anxiety, the short form of the State - Trait Anxiety Inventory, in order to establish that the CSAQ discriminates between general anxiety and anxiety about spatial abilities. 6.3 DATA ANALYSIS OVERVIEW The data can be found in the file csaqp.csv. Each row is a different participant in the study. The first column contains arbitrary ID numbers to identify the participants. The next 8 columns contain the children’s responses to each item of the CSAQ. (note this is real data). The csaq column contains the children’s time 1 CSAQ scores. Jing Tian, Su Dam, and Liz Gunderson calculated each child’s CSAQ score by computing an average score of the eight items. Scores could range from 1 to 5, with higher scores indicating higher levels of spatial anxiety. (note this is real data.) The csaqt2 column is the children’s CSAQ scores when they took the measure the second time one week later. The fourth column is the students’ heart rate, the average beats per minute (BPM), during the 15 minute geography lesson. The next column is the children’s scores on the child version of the Spatial Anxiety Scale (CSAS). This scale consists of 5 items that are rated on a 5-point scale. The sum scores can range between 5 to 20. High scores indicate high levels of spatial anxiety The last column is the children’s scores on the short form of the State - Trait Anxiety Inventory (Marteau &amp; Bekker, 1992). These scores are the sum of 6 statements that are rated on a 4-point scale, with higher scores signifying more general anxiety. Once the data is opened in RStudio, we will first calculate internal reliability statistics. Cronbach’s alpha is the traditional measure psychologists use to assess whether a measurement scale has internal reliability. Cronbach’s alpha is similar to the average correlation of the correlations between each item of the scale, or the average inter-item correlation (AIC). For example, for a 3-item scale the AIC is the average of the correlations between item 1 and 2, item 1 and 3, and item 2 and 3. Ideally AICs should be between .15 to .50 (Clark &amp; Watson, 2019). Anything less than .15 would suggest that the items are not related to each other. Anything over .50 would suggest that the items are redundant. Cronbach’s alpha mathematically combines the AIC and the numbers of items in a scale. The closer a Cronbach’s alpha is to 1, the better a scale’s internal reliability (because the correlations between the scale items are higher). Cronbach’s alphas of over .70 are considered acceptable in psychology. McDonald’s omega is an “up and coming” measure of internal reliability. It is becoming an increasingly popular alternative to Cronbach’s alpha because it takes the hierarchical structure of the data into account (i.e., that the items in a scale are probably not independent from one another). McDonald’s omegas over .70 are considered acceptable (Dunn, Baguley, &amp; Brunseden, 2013). Next assess the test-retest reliability with scatterplots and correlation statistics. Positive and strong (r ≥ .50) correlations between the time 1 and time 2 scores establish good test-retest reliability. Scatterplots and correlation statistics are also used to assess criterion, convergent, and discriminant validity. However, there are not cut-offs and benchmarks that indicate good validity. Instead the scatterplots and correlation statistics are used to establish that the pattern of how a measure is related to other measures makes sense. For example, Children who report high levels of spatial anxiety on the CSAQ should have high heart rates during the geography lesson, while those who report low levels of spatial anxiety should have low heart rates during the lesson. Moreover, a positive association between CSAQ scores and the other measure of spatial anxiety (the CSAS) would indicate convergent validity. Finally, a zero association between the CSAQ scores and general anxiety scores would establish discriminant validity. 6.4 OPEN DATA First assign the dataset to an object called csaqpilot (or whatever you prefer). You can do this with the point and click method by selecting from the environment menu: IMPORT DATASET -&gt; FROM TEXT (READR) If you use this method - remember to change the object name to csaqpilot in the preview box if you want to match my work. Or you could use the following code: library(readr) csaqpilot &lt;- read_csv(\"csaqp.csv\") Then load the tidyverse and psych packages (if they are not loaded already): library(tidyverse) library(psych) 6.5 INTERNAL RELIABILITY Measure the internal reliability of the CSAQ with Cronbach’s alpha and McDonald’s omega. 6.5.1 Cronbach’s Alpha In order to calculate Cronbach’s alpha in R you have to create a new object with only the items of the measurement scale. Do this with the select function of the tidyverse package: csaq &lt;- csaqpilot %&gt;% select(csaqi1, csaqi2, csaqi3, csaqi4, csaqi5, csaqi6, csaqi7, csaqi8) This command tells R to select the variables listed in the select function from the csaqpilot object and save it as csaq. After the command is run, a new object called csaq will appear in your environment. Then use the alpha function of the psych package to compute the Cronbach’s alpha of the 8 items of the CSAQ by putting the csaq subset in the parentheses of the alpha function like this: alpha(csaq) Other packages in R also have functions called alpha (for example the Hmisc and tidyverse package both have alpha functions). When multiple packages with alpha functions are loaded, R can get confused about which alpha function to use. This can be avoided by telling R which package to use. To do this put the package name before the command followed by two colons like this: psych::alpha(csaq) Here is the output of the alpha command: Cronbach’s alpha is reported under the raw alpha. The Cronbach’s alpha of the CSAQ is .60. The next line reports the 95% confidence interval, which is .52 to .69. While the Cronbach’s alpha of the CSAQ is below the .70 cut off that many researchers consider a benchmark for good internal reliability, Cronbach’s alphas with young kids tend to be lower. Internal reliabilities below 0.70 are quite common when studying elementary school-age children. The average_r reports the AIC. In this example the AIC is .16, which is within the acceptable range of .15 to .50. The mean and sd report the respective statistics for the scale if the items are averaged for each participant, which is how the CSAQ is scored. If a scale uses a sum score instead of an average, the numbers reported here would not be the correct mean and sd. The purpose of the ‘Reliability if an item is dropped’ table is to get a sense of what Cronbach’s alpha would be without an item. When Cronbach’s alphas are lower than acceptable, researchers have to revise and reconsider items. Each row of this table reports the Cronbach’s alpha statistics if an item was deleted. For example, the first row reports the Cronbach’s alpha statistics if the first items of the CSAQ (i.e., “How do you feel being asked to say which direction is right or left?”) was dropped. If this item was dropped, the Cronbach’s alpha would be .61. The ‘Item statistics’ table shows how each item of a scale is related to the rest of the scale. The raw.r column reports the correlation between the item and the total score with that item still in it. The r.drop column reports the correlation between the item and the total score without the item. The mean and sd report the means and standard deviations of the scale if that item is dropped. The last table summarizes the proportions of the responses for each item. The CSAQ is on a 5-point scale, so here we can see the proportion of responses that were 1, 2, 3, 4, 5, or missing. For the first item most children selected 5 (44%) and very few children selected 2 (4%). This table is useful for checking the distribution of responses. There should be variability in participants’ responses. The scale would not be meaningful if everyone is giving the same response. 6.5.2 Mcdonald’s Omega The psych package has an omega function, but a package called GPArotation must be installed in order to use it. This package only needs to be installed; you do not need to load it. So, if you have not done so already, first install the package with the following code: (Remember that you only have to install a package once.) install.packages(\"GPArotation\") To compute McDonald’s omega of the 8 items of the CSAQ put the csaq subset in the parentheses of the omega function, followed by nfactors = 1. The nfactors = 1 tells R that you think that the items are all measuring the same thing (spatial anxiety). It means “the number of factors equals one”. Here is what the command should look like: psych::omega(csaq, nfactors = 1) The output of the omega function is long and pulls on advanced statistics, so I suggest focusing on the first table and not worrying about the rest for now. McDonald’s omega can be found in the last row of the first table, next to Omega Total. The McDonald’s omega of the CSAQ scale is .61. Note that this omega function also computes Cronbach’s alpha (see the first row of the table). 6.5.3 TEST-RETEST RELIABILITY We will use scatterplots and correlation coefficients to assess the test-retest reliability of the CSAQ. Let’s first create a scatterplot of the relation between the students’ scores the first time they took the CSAQ and the second time they took it (i.e., the csaq and csaqt2 variables). The command using ggplots of the tidyverse package looks like: ggplot(csaqpilot, aes(x=csaq, y=csaqt2)) + geom_point() csaqpilot tells R to use the data in the csaqpilot object The variables (csaq and t2cmaq) are listed in the aesthetics geom_point tells R that you want a scatterplot Here is the graph: Each data point in the graph is a child. The scatterplot shows that the children who reported that they felt anxious about spatial tasks the first time also reported being anxious the second time. That is, it looks like there is a strong positive correlation. Let’s next calculate a correlation between the time 1 and 2 spatial anxiety scores. We will use the corr.test function to calculate the correlation and its associated statistics. The corr.test function is part of the psych package and the organization of the code below uses tidyverse. Here is the basic command: DataObject %&gt;% select(Variable1Name, Variable2Name) %&gt;% corr.test() %&gt;% print(short=FALSE) Replace DataObject with the name of the object that is storing the data. Replace Variable1Name with the name of one of the variables. Replace Variable2Name with the name of the other variable. Here is the command in the current example: csaqpilot %&gt;% select(csaq, csaqt2) %&gt;% corr.test() %&gt;% print(short=FALSE) The csaqpilot tells R to use the data in the csaqpilot object The select(csaq, csaqt2)tells R to select the variables that we want to include in the correlation matrix. Without this, every variable in the csaqpilot dataset would be included in the correlation matrix. The short = FALSE in the print parentheses prints the confidence intervals Use ?corr.test for more options Here are the results: The first table of the output is the correlation matrix. Each cell in the correlation matrix shows the correlation between two variables. The variable names appear at the top of the columns and the rows. The line of 1.00s is called the diagonal and is the correlation of each variable with itself. The coefficients above and below the diagonal are redundant. The correlation between csaq and csaqt2 is .91. (test-retest reliability with children is rarely this high, outing my fake data). The next table shows the probability associated with the correlation coefficients listed in the first table. These probabilities are the p-values from null hypothesis significance testing (NHST). NHST estimates the likelihood of getting results as extreme or more extreme given the null is true (i.e., given there is really no association between the variables). If this likelihood is sufficiently small (less than 5%), than we reject the null hypothesis and conclude that the association is more extreme than zero. The table reports that the p-values associated with the correlation between csaq and csaqt2 is 0. This means that it is less than .001 (the default is for R to report 0 if the number is sufficiently small to save space - but we know that 0 is never an option in NHST because the tails of the distribution are asymptotic). The last table shows the confidence intervals. The confidence interval provides an interval estimate of a parameter. Here the parameter is the true correlation between two observations. This range fairly often contains the true association (population level) between the variables. In the present example, the correlation coefficient between the time 1 and 2 CSAQ scores (r = 0.91) is a point estimate of the true association between the variables. The confidence interval gives us an interval estimate of this association and it can be found in the first row, that begins with csaq-csqt2. The raw.lower reports the lower bound of the confidence interval, .89. The raw.upper reports the upper bound of the confidence interval, .93. 6.6 CRITERION, CONVERGENT, AND DISCRIMINANT VALIDITY Scatterplots and correlation statistics are also used to establish criterion, convergent, and discriminant validity. Use ggplot to create the scatterplots. Here is the command to create the scatterplot of the participants’ CSAQ scores and their average heart rate during the geography lesson (i.e., the csaq and geobpm variables): ggplot(csaqpilot, aes(x=csaq, y=geobpm)) + geom_point() This plot shows that CSAQ scores are strongly (and positively) correlated to heart rate during a geography lesson. The children with high CSAQ scores had higher heart rates, while those with lower CSAQ scores had lower hear rates. Next use the following command to create the scatterplot of the participants’ CSAQ and CSAS scores: ggplot(csaqpilot, aes(x=csaq, y=csas)) + geom_point() The figure shows a positive relation between the CSAQ and CSAS scores. Children who reported high spatial anxiety on the CSAQ also reported high spatial anxiety on the CSAS, and vice-versa. Finally, here is the command to create the scatterplot of the participants’ CSAQ and general anxiety scores: ggplot(csaqpilot, aes(x=csaq, y=genanx)) + geom_point() The graph shows a zero association between CSAQ scores and general anxiety scores. Some of the children who reported high levels of spatial anxiety also reported high levels of general anxiety while others reported low levels of general anxiety. Similarly, some of the children who reported low levels of spatial anxiety reported high levels of general anxiety while others reported low levels. There is no clear pattern. Next calculate correlation statistics for the relation between the CSAQ scores, the geography lesson heart rate, the CSAS scores, and the general anxiety scores. Here is the command to create the correlation matrix: csaqpilot %&gt;% select(csaq, geobpm, csas, genanx) %&gt;% corr.test() %&gt;% print(short=FALSE) ## Call:corr.test(x = .) ## Correlation matrix ## csaq geobpm csas genanx ## csaq 1.00 0.90 0.92 -0.04 ## geobpm 0.90 1.00 0.81 0.04 ## csas 0.92 0.81 1.00 -0.06 ## genanx -0.04 0.04 -0.06 1.00 ## Sample Size ## [1] 198 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## csaq geobpm csas genanx ## csaq 0.00 0.00 0.00 1 ## geobpm 0.00 0.00 0.00 1 ## csas 0.00 0.00 0.00 1 ## genanx 0.55 0.58 0.39 0 ## ## Confidence intervals based upon normal theory. To get bootstrapped values, try cor.ci ## raw.lower raw.r raw.upper raw.p lower.adj upper.adj ## csaq-gebpm 0.87 0.90 0.93 0.00 0.86 0.93 ## csaq-csas 0.90 0.92 0.94 0.00 0.89 0.95 ## csaq-gennx -0.18 -0.04 0.10 0.55 -0.20 0.12 ## gebpm-csas 0.75 0.81 0.85 0.00 0.74 0.86 ## gebpm-gennx -0.10 0.04 0.18 0.58 -0.10 0.18 ## csas-gennx -0.20 -0.06 0.08 0.39 -0.23 0.11 A correlation matrix with more than two variables is interpreted in the same way as those with two variables. Each cell contains correlation statistics between the variable listed at the top of the column and the beginning of the row. In this example, we are interested in the correlations between the children CSAQ scores and the geography lesson heart rate, the CSAS scores, and the general anxiety scores. So, focus on the csaq column. The correlation between the csaq and geobpm variables is .90. The correlation between the csaq and csas variables is .92. Finally, the correlation between the csaq and genanx variables is -.04. The next table shows the probabilities associated with the correlation coefficients listed in the first table. Again, focus on the csaq column. The results show that the p-values associated with the correlations between the csaq and geobpm variables and the csaq and csas variables are 0, which means that they are less than .001 - so these associations are statistically significant. The probability value associated with correlation between the csaq and genanx variables is .55, which is above the .05 cutoff for statistical significance. The last table shows the confidence intervals. Here the statistics for each correlation are listed in each row. The first column shows the variable names – sometimes R drops the vowels in the names here to save space). In this example the first row reports statistics for the relationship between the csaq and geobpm variables. The lower bound of the confidence interval is .87 (under the raw.lower) and the upper bound is .93 (under raw.upper). The second row reports the confidence interval for the correlation between the csaq and csas variables, which is .90 to .94. The third row reports the confidence interval for the correlation between the csaq and genanx variables, which is -0.18 to .10. Note that this confidence interval includes zero, which is consistent with NHST because both are saying that zero is a likely correlation between the variables. 6.7 APA-STYLE WRITE-UP Spatial anxiety was measured with the Child Spatial Anxiety Questionnaire (CSAQ). Items ask children how nervous they would feel during situations involving spatial tasks (e.g., “How do you feel being asked to say which direction is right or left?”). Children responded using five smiley faces displaying emotions ranging from “not nervous at all” to “very, very nervous” (1–5 scale). Internal reliability was moderate (Cronbach’s alpha (⍺) = .60, 95% CI = .52 to .69; McDonald’s Omega (⍵) = .61). The children were administered the CSAQ two times – 1 week apart. Test-retest reliability was high (r = .91, p &lt; .001, 95% CI = .89 to .93). The validity of the CSAQ was also considered. CSAQ scores were positively related to children’s heart rate during a geography lesson (r = .90, p &lt; .001, 95% CI = .87 to .93), establishing criterion validity of the CSAQ. CSAQ scores were positively related to another measure of spatial anxiety, the CSAS (r = .92, p &lt; .001, 95% CI = .90 to .94), establishing convergent validity. Moreover, the CSAQ scores were not related to general anxiety scores (r = -.04, p = . 55, CI.95 = -0.18 to .10), establishing discriminant validity. "]]
